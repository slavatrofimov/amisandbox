{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fed6147-755d-461a-8a56-75e6aaab935b",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Deploy solution\n",
    "\n",
    "This notebook deploys solution accelerator to the specified workspace.\n",
    "\n",
    "**What is happening in this notebook?**\n",
    " - It downloads the latest set of source files from Github\n",
    " - It deploys/updates the Fabric items in the current workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7439a740-1fc5-4e3c-a47e-de324036912a",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.statement-meta+json": {
       "execution_finish_time": "2025-10-23T19:07:33.2812183Z",
       "execution_start_time": "2025-10-23T19:07:27.6156285Z",
       "normalized_state": "finished",
       "parent_msg_id": "cf67169a-41e9-4406-8c15-f9c953b45435",
       "queued_time": "2025-10-23T19:07:19.2061066Z",
       "session_id": "3bad4c2a-b07e-46ab-a1f6-19d459bb3ebe",
       "session_start_time": "2025-10-23T19:07:19.2071076Z"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ms-fabric-cli\n",
      "  Downloading ms_fabric_cli-1.2.0-py3-none-any.whl.metadata (9.8 kB)\n",
      "Requirement already satisfied: msal<2,>=1.29 in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from msal[broker]<2,>=1.29->ms-fabric-cli) (1.32.3)\n",
      "Requirement already satisfied: msal_extensions in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from ms-fabric-cli) (1.3.1)\n",
      "Collecting questionary (from ms-fabric-cli)\n",
      "  Downloading questionary-2.1.1-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: prompt_toolkit>=3.0.41 in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from ms-fabric-cli) (3.0.51)\n",
      "Requirement already satisfied: cachetools>=5.5.0 in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from ms-fabric-cli) (5.5.2)\n",
      "Requirement already satisfied: jmespath in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from ms-fabric-cli) (1.0.1)\n",
      "Requirement already satisfied: pyyaml==6.0.2 in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from ms-fabric-cli) (6.0.2)\n",
      "Collecting argcomplete>=3.6.2 (from ms-fabric-cli)\n",
      "  Downloading argcomplete-3.6.3-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: psutil==7.0.0 in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from ms-fabric-cli) (7.0.0)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from msal<2,>=1.29->msal[broker]<2,>=1.29->ms-fabric-cli) (2.32.3)\n",
      "Requirement already satisfied: PyJWT<3,>=1.0.0 in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from PyJWT[crypto]<3,>=1.0.0->msal<2,>=1.29->msal[broker]<2,>=1.29->ms-fabric-cli) (2.8.0)\n",
      "Requirement already satisfied: cryptography<47,>=2.5 in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from msal<2,>=1.29->msal[broker]<2,>=1.29->ms-fabric-cli) (45.0.3)\n",
      "Requirement already satisfied: wcwidth in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from prompt_toolkit>=3.0.41->ms-fabric-cli) (0.2.13)\n",
      "Requirement already satisfied: cffi>=1.14 in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from cryptography<47,>=2.5->msal<2,>=1.29->msal[broker]<2,>=1.29->ms-fabric-cli) (1.17.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from requests<3,>=2.0.0->msal<2,>=1.29->msal[broker]<2,>=1.29->ms-fabric-cli) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from requests<3,>=2.0.0->msal<2,>=1.29->msal[broker]<2,>=1.29->ms-fabric-cli) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from requests<3,>=2.0.0->msal<2,>=1.29->msal[broker]<2,>=1.29->ms-fabric-cli) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from requests<3,>=2.0.0->msal<2,>=1.29->msal[broker]<2,>=1.29->ms-fabric-cli) (2024.7.4)\n",
      "Requirement already satisfied: pycparser in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from cffi>=1.14->cryptography<47,>=2.5->msal<2,>=1.29->msal[broker]<2,>=1.29->ms-fabric-cli) (2.22)\n",
      "Downloading ms_fabric_cli-1.2.0-py3-none-any.whl (305 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m305.1/305.1 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading argcomplete-3.6.3-py3-none-any.whl (43 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading questionary-2.1.1-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: argcomplete, questionary, ms-fabric-cli\n",
      "Successfully installed argcomplete-3.6.3 ms-fabric-cli-1.2.0 questionary-2.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ms-fabric-cli"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb77f4b-4cc8-49c6-83cd-231680a1d618",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Import of needed libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414e6feb-ec15-4bb6-b111-5558df98fea0",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.statement-meta+json": {
       "execution_finish_time": "2025-10-23T19:07:37.3690516Z",
       "execution_start_time": "2025-10-23T19:07:33.2824513Z",
       "normalized_state": "finished",
       "parent_msg_id": "05513590-62af-46f6-a353-f13685cc821c",
       "queued_time": "2025-10-23T19:07:19.2094598Z",
       "session_id": "3bad4c2a-b07e-46ab-a1f6-19d459bb3ebe",
       "session_start_time": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import json\n",
    "from zipfile import ZipFile \n",
    "import shutil\n",
    "import re\n",
    "import requests\n",
    "import zipfile\n",
    "from io import BytesIO\n",
    "import yaml\n",
    "import time\n",
    "import sempy.fabric as fabric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44471f8d-0f6f-4d86-9bef-20a0afccd13e",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Download of source & config files\n",
    "This part downloads all source and config files of AMI needed for the deployment into the ressources of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70bfa16b-c718-48d6-81b1-00f456ccc80d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.statement-meta+json": {
       "execution_finish_time": "2025-10-23T19:07:39.3819514Z",
       "execution_start_time": "2025-10-23T19:07:37.3703544Z",
       "normalized_state": "finished",
       "parent_msg_id": "b8e27e19-8aea-464c-a288-a52f8a6d64eb",
       "queued_time": "2025-10-23T19:07:19.2116522Z",
       "session_id": "3bad4c2a-b07e-46ab-a1f6-19d459bb3ebe",
       "session_start_time": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def download_folder_as_zip(repo_owner, repo_name, output_zip, branch=\"main\", folder_to_extract=\"workspace\",  remove_folder_prefix = \"\"):\n",
    "    # Construct the URL for the GitHub API to download the repository as a zip file\n",
    "    url = f\"https://api.github.com/repos/{repo_owner}/{repo_name}/zipball/{branch}\"\n",
    "    \n",
    "    # Make a request to the GitHub API\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    # Ensure the directory for the output zip file exists\n",
    "    os.makedirs(os.path.dirname(output_zip), exist_ok=True)\n",
    "    \n",
    "    # Create a zip file in memory\n",
    "    with zipfile.ZipFile(BytesIO(response.content)) as zipf:\n",
    "        with zipfile.ZipFile(output_zip, 'w') as output_zipf:\n",
    "            for file_info in zipf.infolist():\n",
    "                parts = file_info.filename.split('/')\n",
    "                if  re.sub(r'^.*?/', '/', file_info.filename).startswith(folder_to_extract): \n",
    "                    # Extract only the specified folder\n",
    "                    file_data = zipf.read(file_info.filename)\n",
    "                    output_zipf.writestr(('/'.join(parts[1:]).replace(remove_folder_prefix, \"\")), file_data)\n",
    "\n",
    "def uncompress_zip_to_folder(zip_path, extract_to):\n",
    "    # Ensure the directory for extraction exists\n",
    "    os.makedirs(extract_to, exist_ok=True)\n",
    "    \n",
    "    # Uncompress all files from the zip into the specified folder\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "    \n",
    "    # Delete the original zip file\n",
    "    os.remove(zip_path)\n",
    "\n",
    "repo_owner = \"slavatrofimov\"\n",
    "repo_name = \"amisandbox\"\n",
    "branch = \"main\"\n",
    "folder_prefix = \"\"\n",
    "\n",
    "download_folder_as_zip(repo_owner, repo_name, output_zip = \"./builtin/workspace/workspace.zip\", branch = branch, folder_to_extract= \"/workspace\", remove_folder_prefix = folder_prefix)\n",
    "download_folder_as_zip(repo_owner, repo_name, output_zip = \"./builtin/config/config.zip\", branch = branch, folder_to_extract= \"/config\" , remove_folder_prefix = folder_prefix)\n",
    "\n",
    "uncompress_zip_to_folder(zip_path = \"./builtin/config/config.zip\", extract_to= \"./builtin\")\n",
    "#uncompress_zip_to_folder(zip_path = \"./builtin/workspace/workspace.zip\", extract_to= \"./builtin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eeef159d-0f52-43a1-a895-5c3b293ffc61",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.statement-meta+json": {
       "execution_finish_time": "2025-10-23T19:07:39.7107518Z",
       "execution_start_time": "2025-10-23T19:07:39.3831718Z",
       "normalized_state": "finished",
       "parent_msg_id": "bee54907-b318-4bdd-bcd7-b50edd1b534f",
       "queued_time": "2025-10-23T19:07:19.2130614Z",
       "session_id": "3bad4c2a-b07e-46ab-a1f6-19d459bb3ebe",
       "session_start_time": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_path = './builtin/'\n",
    "\n",
    "deploy_order_path = os.path.join(base_path, 'config/deployment_sequence.json')\n",
    "with open(deploy_order_path, 'r') as file:\n",
    "        deployment_order = json.load(file)\n",
    "\n",
    "\n",
    "deployment_order = [\n",
    "        {\n",
    "        \"name\": \"ReferenceDataLH.Lakehouse\",\n",
    "        \"folder\": \"Store and Query\",\n",
    "        \"id\": \"08d5aa3f-8374-9b1f-4a32-cbc7374bb522\"\n",
    "        },\n",
    "        {\n",
    "        \"name\": \"AMI Telemetry and Outage Simulation.Notebook\",\n",
    "        \"folder\": \"Simulation\",\n",
    "        \"id\": \"735054a9-b72c-a2c0-4aa2-197db7d80dd8\"\n",
    "        },\n",
    "\n",
    "\n",
    "        ]\n",
    "\n",
    "mapping_table=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4b4640-8237-433b-ac46-8986531f28ce",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Definition of deployment functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bba6a51-107d-4c10-9a41-5cb2a9e07c27",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.statement-meta+json": {
       "execution_finish_time": "2025-10-23T19:27:53.0169448Z",
       "execution_start_time": "2025-10-23T19:27:52.6844932Z",
       "normalized_state": "finished",
       "parent_msg_id": "21698d61-9357-455d-89a6-2019408b39b1",
       "queued_time": "2025-10-23T19:27:52.6834549Z",
       "session_id": "3bad4c2a-b07e-46ab-a1f6-19d459bb3ebe",
       "session_start_time": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set environment parameters for Fabric CLI\n",
    "token = notebookutils.credentials.getToken('pbi')\n",
    "os.environ['FAB_TOKEN'] = token\n",
    "os.environ['FAB_TOKEN_ONELAKE'] = token\n",
    "\n",
    "def run_fab_command( command, capture_output: bool = False, silently_continue: bool = False):\n",
    "    result = subprocess.run([\"fab\", \"-c\", command], capture_output=capture_output, text=True)\n",
    "    if (not(silently_continue) and (result.returncode > 0 or result.stderr)):\n",
    "       raise Exception(f\"Error running fab command. exit_code: '{result.returncode}'; stderr: '{result.stderr}'\")    \n",
    "    if (capture_output): \n",
    "        output = result.stdout.strip()\n",
    "        return output\n",
    "\n",
    "def fab_get_id(name):\n",
    "    id = run_fab_command(f\"get /{trg_workspace_name}.Workspace/{name} -q id\" , capture_output = True, silently_continue= True)\n",
    "    return(id)\n",
    "\n",
    "def get_id_by_name(name):\n",
    "    for it in deployment_order:\n",
    "        if it.get(\"name\") == name:\n",
    "                return it.get(\"id\")\n",
    "    return None\n",
    "\n",
    "def copy_to_tmp(name, folder):\n",
    "    shutil.rmtree(\"./builtin/tmp\",  ignore_errors=True)\n",
    "    path2zip = \"./builtin/workspace/workspace.zip\"\n",
    "    with  ZipFile(path2zip) as archive:\n",
    "        for file in archive.namelist():\n",
    "            if file.startswith(f'workspace/{folder}{name}/'):    \n",
    "                archive.extract(file, './builtin/tmp')\n",
    "    return(f\"./builtin/tmp/workspace/{folder}{name}\")\n",
    "\n",
    "def deploy_kql_database(eventhouse_name, kql_db_path, kql_db_name, parent_eventhouse_id):\n",
    "    \"\"\"\n",
    "    Deploy a KQL Database as a child item of an Eventhouse using Fabric API.\n",
    "    \n",
    "    Args:\n",
    "        eventhouse_name: Name of the parent Eventhouse\n",
    "        kql_db_path: Path to the KQL database folder containing DatabaseProperties.json and DatabaseSchema.kql\n",
    "        kql_db_name: Name of the KQL database to create\n",
    "        parent_eventhouse_id: ID of the parent Eventhouse\n",
    "    \n",
    "    Returns:\n",
    "        The ID of the created KQL database\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read DatabaseProperties.json to get configuration\n",
    "        db_properties_path = os.path.join(kql_db_path, 'DatabaseProperties.json')\n",
    "        if os.path.exists(db_properties_path):\n",
    "            with open(db_properties_path, 'r', encoding='utf-8') as file:\n",
    "                db_properties = json.load(file)\n",
    "        else:\n",
    "            # Default properties if file doesn't exist\n",
    "            db_properties = {\n",
    "                \"databaseType\": \"ReadWrite\",\n",
    "                \"oneLakeCachingPeriod\": \"P36500D\",\n",
    "                \"oneLakeStandardStoragePeriod\": \"P36500D\"\n",
    "            }\n",
    "        \n",
    "        # Update parent Eventhouse ID\n",
    "        db_properties[\"parentEventhouseItemId\"] = parent_eventhouse_id\n",
    "        \n",
    "        # Create KQL Database using Fabric API\n",
    "        create_payload = {\n",
    "            \"displayName\": kql_db_name,\n",
    "            \"type\": \"KQLDatabase\",\n",
    "            \"description\": f\"KQL Database {kql_db_name}\",\n",
    "            \"definition\": {\n",
    "                \"parts\": [\n",
    "                    {\n",
    "                        \"path\": \"DatabaseProperties.json\",\n",
    "                        \"payload\": json.dumps(db_properties, indent=2),\n",
    "                        \"payloadType\": \"InlineBase64\"\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Convert payload to JSON string for the API call\n",
    "        create_payload_json = json.dumps(create_payload).replace('\"', '\\\\\"')\n",
    "        \n",
    "        print(f\"Creating KQL Database: {kql_db_name}\")\n",
    "        \n",
    "        # Create the KQL database using Fabric API\n",
    "        create_result = run_fab_command(\n",
    "            f'api -A fabric -X post workspaces/{trg_workspace_id}/items -i \"{create_payload_json}\"',\n",
    "            capture_output=True,\n",
    "            silently_continue=True\n",
    "        )\n",
    "        \n",
    "        if create_result:\n",
    "            result_json = json.loads(create_result)\n",
    "            kql_db_id = result_json.get('text', {}).get('id')\n",
    "            print(f\"Created KQL Database with ID: {kql_db_id}\")\n",
    "        else:\n",
    "            # Try to get the ID if it already exists\n",
    "            kql_db_id = fab_get_id(f\"{eventhouse_name}/{kql_db_name}\")\n",
    "            print(f\"KQL Database may already exist, retrieved ID: {kql_db_id}\")\n",
    "        \n",
    "        # Deploy schema if DatabaseSchema.kql exists\n",
    "        schema_path = os.path.join(kql_db_path, 'DatabaseSchema.kql')\n",
    "        if os.path.exists(schema_path):\n",
    "            print(f\"Deploying schema from: {schema_path}\")\n",
    "            \n",
    "            with open(schema_path, 'r', encoding='utf-8') as file:\n",
    "                schema_content = file.read()\n",
    "            \n",
    "            # Execute the KQL schema commands\n",
    "            if schema_content.strip():\n",
    "                # Split schema into individual commands and execute them\n",
    "                kql_commands = [cmd.strip() for cmd in schema_content.split('\\n') if cmd.strip() and not cmd.strip().startswith('//')]\n",
    "                \n",
    "                for i, command in enumerate(kql_commands):\n",
    "                    if command:\n",
    "                        try:\n",
    "                            print(f\"Executing KQL command {i+1}/{len(kql_commands)}\")\n",
    "                            \n",
    "                            # Prepare the KQL command payload\n",
    "                            kql_payload = {\n",
    "                                \"csl\": command,\n",
    "                                \"db\": kql_db_name\n",
    "                            }\n",
    "                            \n",
    "                            kql_payload_json = json.dumps(kql_payload).replace('\"', '\\\\\"')\n",
    "                            \n",
    "                            # Execute KQL command using Fabric API\n",
    "                            run_fab_command(\n",
    "                                f'api -A fabric -X post workspaces/{trg_workspace_id}/items/{kql_db_id}/runKqlCommand -i \"{kql_payload_json}\"',\n",
    "                                capture_output=False,\n",
    "                                silently_continue=True\n",
    "                            )\n",
    "                            \n",
    "                        except Exception as e:\n",
    "                            print(f\"Warning: Failed to execute KQL command {i+1}: {str(e)}\")\n",
    "                            continue\n",
    "        \n",
    "        return kql_db_id\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error deploying KQL Database {kql_db_name}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def replace_ids_in_folder(folder_path, mapping_table):\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for file_name in files:\n",
    "            if file_name.endswith(('.py', '.json', '.pbir', '.platform', '.ipynb', '.tmdl')) and not file_name.endswith('report.json'):\n",
    "                file_path = os.path.join(root, file_name)\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    content = file.read()\n",
    "                    for mapping in mapping_table:  \n",
    "                        content = content.replace(mapping[\"old_id\"], mapping[\"new_id\"])\n",
    "                with open(file_path, 'w', encoding='utf-8') as file:\n",
    "                    file.write(content)\n",
    "\n",
    "def replace_githubRef_in_folder(folder_path, repo_owner, repo_name, branch):\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for file_name in files:\n",
    "            if file_name.endswith(('.py', '.json', '.pbir', '.platform', '.ipynb')) and not file_name.endswith('report.json'):\n",
    "                file_path = os.path.join(root, file_name)\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    content = file.read()\n",
    "                    matches = re.findall(r'https://raw\\.githubusercontent\\.com/([^/]+/[^/]+/[^/]+/[^/]+/[^/]+)/(.*)', content, re.MULTILINE)\n",
    "                    if matches:\n",
    "                        for match in matches:\n",
    "                            oldUrl = f\"https://raw.githubusercontent.com/{match[0]}/{match[1]}\"\n",
    "                            newUrl = f\"https://raw.githubusercontent.com/{repo_owner}/{repo_name}/refs/heads/{branch}/{match[1]}\"\n",
    "                            content = content.replace(oldUrl, newUrl)\n",
    "                        with open(file_path, 'w', encoding='utf-8') as file:\n",
    "                                file.write(content)\n",
    "\n",
    "def get_semantic_model_id(report_folder):\n",
    "    definition_file = os.path.join(report_folder, 'definition.pbir')\n",
    "    if os.path.exists(definition_file):\n",
    "        with open(definition_file, 'r', encoding='utf-8') as file:\n",
    "            content = json.load(file)\n",
    "            semantic_model_id = content.get('datasetReference', {}).get('byConnection', {}).get('pbiModelDatabaseName')\n",
    "            if semantic_model_id:\n",
    "                return semantic_model_id\n",
    "    return None\n",
    "\n",
    "def update_sm_connection_to_ami_lakehouse(semantic_model_folder):\n",
    "    new_sm_db= run_fab_command(f\"get /{trg_workspace_name}.Workspace/AMI.Lakehouse -q properties.sqlEndpointProperties.connectionString\", capture_output = True, silently_continue=True)\n",
    "    new_lakehouse_sql_id= run_fab_command(f\"get /{trg_workspace_name}.Workspace/AMI.Lakehouse -q properties.sqlEndpointProperties.id\", capture_output = True, silently_continue=True)\n",
    "        \n",
    "    expressions_file = os.path.join(semantic_model_folder, 'definition', 'expressions.tmdl')\n",
    "    if os.path.exists(expressions_file):\n",
    "        with open(expressions_file, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "            match = re.search(r'Sql\\.Database\\(\"([^\"]+)\",\\s*\"([^\"]+)\"\\)', content)\n",
    "            if match:\n",
    "                old_sm_db, old_lakehouse_sql_id = match.group(1), match.group(2)\n",
    "                content = content.replace(old_sm_db, new_sm_db).replace(old_lakehouse_sql_id, new_lakehouse_sql_id)\n",
    "                with open(expressions_file, 'w', encoding='utf-8') as file:\n",
    "                    file.write(content)\n",
    "\n",
    "def update_report_definition( path): \n",
    "    semantic_model_id = get_semantic_model_id(path)\n",
    "    definition_path = os.path.join(path, \"definition.pbir\")\n",
    "   \n",
    "    with open(definition_path, \"r\", encoding=\"utf8\") as file:\n",
    "        report_definition = json.load(file)\n",
    "\n",
    "    report_definition[\"datasetReference\"][\"byPath\"] = None\n",
    "\n",
    "    by_connection_obj = {\n",
    "            \"connectionString\": None,\n",
    "            \"pbiServiceModelId\": None,\n",
    "            \"pbiModelVirtualServerName\": \"sobe_wowvirtualserver\",\n",
    "            \"pbiModelDatabaseName\": semantic_model_id,\n",
    "            \"name\": \"EntityDataSource\",\n",
    "            \"connectionType\": \"pbiServiceXmlaStyleLive\",\n",
    "        }\n",
    "\n",
    "    report_definition[\"datasetReference\"][\"byConnection\"] = by_connection_obj\n",
    "\n",
    "    with open(definition_path, \"w\") as file:\n",
    "            json.dump(report_definition, file, indent=4)\n",
    "\n",
    "def print_color(text, state):\n",
    "    red  = '\\033[91m'\n",
    "    yellow = '\\033[93m'  \n",
    "    green = '\\033[92m'   \n",
    "    white = '\\033[0m'  \n",
    "    if state == \"error\":\n",
    "        print(red, text, white)\n",
    "    elif state == \"warning\":\n",
    "        print(yellow, text, white)\n",
    "    elif state == \"success\":\n",
    "        print(green, text, white)\n",
    "    else:\n",
    "        print(\"\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4473b6db-df34-4a72-bd1a-05d5ddefa78e",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Get current Workspace\n",
    "This cell gets the current workspace to deploy AMI automatically inside it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d54542c-1e34-434b-bada-d73fbe7c2535",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.statement-meta+json": {
       "execution_finish_time": "2025-10-23T19:07:41.3060154Z",
       "execution_start_time": "2025-10-23T19:07:40.0557897Z",
       "normalized_state": "finished",
       "parent_msg_id": "ebc6842b-ec41-42fa-8431-6483d80de8bd",
       "queued_time": "2025-10-23T19:07:19.2160095Z",
       "session_id": "3bad4c2a-b07e-46ab-a1f6-19d459bb3ebe",
       "session_start_time": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current workspace: ST-AMI Sandbox\n",
      "Current workspace ID: 57347e55-c195-4e9b-96b8-03c4703f23c2\n"
     ]
    }
   ],
   "source": [
    "trg_workspace_id = fabric.get_notebook_workspace_id()\n",
    "res = run_fab_command(f\"api -X get workspaces/{trg_workspace_id}\" , capture_output = True, silently_continue=True)\n",
    "trg_workspace_name = json.loads(res)[\"text\"][\"displayName\"]\n",
    "\n",
    "print(f\"Current workspace: {trg_workspace_name}\")\n",
    "print(f\"Current workspace ID: {trg_workspace_id}\")\n",
    "\n",
    "mapping_table.append({ \"old_id\": get_id_by_name(\"Focus\"), \"new_id\": trg_workspace_id })\n",
    "mapping_table.append({ \"old_id\": \"00000000-0000-0000-0000-000000000000\", \"new_id\": trg_workspace_id })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8628a42-1d8a-4192-8e75-4754e626fede",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Deployment Logic\n",
    "This part iterates through all the items, gets the respective source code, replaces all IDs dynamically and deploys the new item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "398e8db0-fedf-464b-a228-c5c623a57f9e",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.statement-meta+json": {
       "execution_finish_time": "2025-10-23T19:07:41.6270057Z",
       "execution_start_time": "2025-10-23T19:07:41.3072367Z",
       "normalized_state": "finished",
       "parent_msg_id": "db081cc6-40dc-48e9-badc-ae7aff003996",
       "queued_time": "2025-10-23T19:07:19.217433Z",
       "session_id": "3bad4c2a-b07e-46ab-a1f6-19d459bb3ebe",
       "session_start_time": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./builtin/tmp/workspace/Simulation/AMI Telemetry and Outage Simulation.Notebook\n"
     ]
    }
   ],
   "source": [
    "tmp_path = copy_to_tmp(\"AMI Telemetry and Outage Simulation.Notebook\", \"Simulation/\")\n",
    "print(tmp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315ef8af-ff5a-4323-869c-7198e910c0bf",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.statement-meta+json": {
       "execution_finish_time": "2025-10-23T19:30:24.7739099Z",
       "execution_start_time": "2025-10-23T19:30:24.4220188Z",
       "normalized_state": "finished",
       "parent_msg_id": "fdef3771-4497-4f28-805d-e0c0da6f44d4",
       "queued_time": "2025-10-23T19:30:24.4210354Z",
       "session_id": "3bad4c2a-b07e-46ab-a1f6-19d459bb3ebe",
       "session_start_time": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Store and Query/\n",
      "ReferenceDataLH.Lakehouse\n",
      "\n",
      "#############################################\n",
      "Deploying Store and Query/ReferenceDataLH.Lakehouse\n",
      "./builtin/tmp/workspace/Store and Query/ReferenceDataLH.Lakehouse\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "replace() argument 1 must be str, not None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m tmp_path \u001b[38;5;241m=\u001b[39m copy_to_tmp(name, folder)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(tmp_path)\n\u001b[0;32m---> 24\u001b[0m \u001b[43mreplace_ids_in_folder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtmp_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapping_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m replace_githubRef_in_folder(tmp_path,repo_owner,repo_name,branch)\n\u001b[1;32m     27\u001b[0m cli_parameter \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Cell \u001b[0;32mIn[13], line 41\u001b[0m, in \u001b[0;36mreplace_ids_in_folder\u001b[0;34m(folder_path, mapping_table)\u001b[0m\n\u001b[1;32m     39\u001b[0m     content \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m mapping \u001b[38;5;129;01min\u001b[39;00m mapping_table:  \n\u001b[0;32m---> 41\u001b[0m         content \u001b[38;5;241m=\u001b[39m \u001b[43mcontent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mold_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnew_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m     43\u001b[0m     file\u001b[38;5;241m.\u001b[39mwrite(content)\n",
      "\u001b[0;31mTypeError\u001b[0m: replace() argument 1 must be str, not None"
     ]
    }
   ],
   "source": [
    "exclude = [\"Focus\", 'Deploy_AMI']\n",
    "\n",
    "for it in deployment_order:\n",
    "\n",
    "    new_id = None\n",
    "    \n",
    "    name = it[\"name\"]\n",
    "    folder = \"\" if it[\"folder\"] == \"\" else it[\"folder\"] + \"/\"\n",
    "\n",
    "    print(folder)\n",
    "    print(name)\n",
    "\n",
    "\n",
    "    if name in exclude:\n",
    "            continue\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"#############################################\")\n",
    "    print(f\"Deploying {folder}{name}\")\n",
    "\n",
    "    # Copy and replace IDs in the item\n",
    "    tmp_path = copy_to_tmp(name, folder)\n",
    "    print(tmp_path)\n",
    "    replace_ids_in_folder(tmp_path, mapping_table)\n",
    "    replace_githubRef_in_folder(tmp_path,repo_owner,repo_name,branch)\n",
    "\n",
    "    cli_parameter = ''\n",
    "    if \".Notebook\" in name:\n",
    "        cli_parameter = cli_parameter + \" --format .ipynb\"\n",
    "    elif \".Lakehouse\" in name:\n",
    "        run_fab_command(f\"create /{trg_workspace_name}.Workspace/{name}\" , silently_continue=True )\n",
    "        new_id = fab_get_id(name)\n",
    "        mapping_table.append({ \"old_id\": get_id_by_name(name), \"new_id\": new_id })\n",
    "        \n",
    "        continue\n",
    "    elif \".Eventhouse\" in name:\n",
    "        # Deploy Eventhouse first (this creates the container)\n",
    "        run_fab_command(f\"import /{trg_workspace_name}.Workspace/{name} -i {tmp_path}\", silently_continue=True)\n",
    "        new_id = fab_get_id(name)\n",
    "        mapping_table.append({ \"old_id\": it[\"id\"], \"new_id\": new_id })\n",
    "        \n",
    "        # Check for KQL Database children and deploy them\n",
    "        children_path = os.path.join(tmp_path, '.children')\n",
    "        if os.path.exists(children_path):\n",
    "            for child_item in os.listdir(children_path):\n",
    "                if child_item.endswith('.KQLDatabase'):\n",
    "                    kql_db_path = os.path.join(children_path, child_item)\n",
    "                    kql_db_name = child_item.replace('.KQLDatabase', '')\n",
    "                    \n",
    "                    print(f\"Deploying KQL Database: {kql_db_name}\")\n",
    "                    kql_db_id = deploy_kql_database(name.replace('.Eventhouse', ''), kql_db_path, kql_db_name, new_id)\n",
    "                    \n",
    "                    if kql_db_id:\n",
    "                        # Add mapping for the KQL database if we have its original ID\n",
    "                        kql_platform_file = os.path.join(kql_db_path, '.platform')\n",
    "                        if os.path.exists(kql_platform_file):\n",
    "                            with open(kql_platform_file, 'r', encoding='utf-8') as file:\n",
    "                                platform_data = json.load(file)\n",
    "                                old_kql_id = platform_data.get('config', {}).get('logicalId')\n",
    "                                if old_kql_id:\n",
    "                                    mapping_table.append({ \"old_id\": old_kql_id, \"new_id\": kql_db_id })\n",
    "        \n",
    "        continue\n",
    "    elif \".Report\" in name:\n",
    "        update_report_definition(  tmp_path  )\n",
    "    elif \".SemanticModel\" in name:\n",
    "        update_sm_connection_to_ami_lakehouse(tmp_path)\n",
    "    \n",
    "    run_fab_command(f\"import  /{trg_workspace_name}.Workspace/{name} -i {tmp_path} -f {cli_parameter} \", silently_continue= True)\n",
    "    new_id= fab_get_id(name)\n",
    "    mapping_table.append({ \"old_id\": it[\"id\"], \"new_id\": new_id })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5421cec4-acc4-4940-9d5f-12bd9a7de336",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Post-Deployment logic\n",
    "In this separate notebook, all needed tables for AMI are automatically deployed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9408aba-636a-41a8-9a16-aa1945150a5c",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.statement-meta+json": {
       "execution_finish_time": null,
       "execution_start_time": null,
       "normalized_state": "cancelling",
       "parent_msg_id": "81d8d7c8-20b1-48a5-b381-a72330c46da1",
       "queued_time": "2025-10-23T19:07:19.2200826Z",
       "session_id": "3bad4c2a-b07e-46ab-a1f6-19d459bb3ebe",
       "session_start_time": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fill default tables\n",
    "time.sleep(10)\n",
    "run_fab_command('job run ' + trg_workspace_name + '.Workspace/Init_AMI_Lakehouse_Tables.Notebook -i {\"parameters\": {\"_inlineInstallationEnabled\": {\"type\": \"Bool\", \"value\": \"True\"} } }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed492f8-14dd-4078-baf6-afcdea407340",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.statement-meta+json": {
       "execution_finish_time": null,
       "execution_start_time": null,
       "normalized_state": "cancelling",
       "parent_msg_id": "645e186c-def5-4f6e-bb7f-760ab182beba",
       "queued_time": "2025-10-23T19:07:19.2214772Z",
       "session_id": "3bad4c2a-b07e-46ab-a1f6-19d459bb3ebe",
       "session_start_time": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Refresh SQL Endpoint for Config_Lakehouse\n",
    "items = run_fab_command(f'api -X get -A fabric /workspaces/{trg_workspace_id}/items' , capture_output = True)\n",
    "for it in json.loads(items)['text']['value']:\n",
    "    if (it['displayName'] == 'AMI' ) & (it['type'] =='SQLEndpoint' ):\n",
    "        lh_sql_endpoint = it['id']\n",
    "print(f\"AMI Lakehouse SQL Endpoint ID: {lh_sql_endpoint}\")\n",
    "\n",
    "try:\n",
    "    run_fab_command(f'api -A fabric -X post workspaces/{trg_workspace_id}/sqlEndpoints/{lh_sql_endpoint}/refreshMetadata?preview=True -i {{}} ', capture_output=True)\n",
    "    print(\"Refresh AMI_SQL_Endpoint\")\n",
    "except:\n",
    "    print(\"SQL Endpoint Refresh API failed, it is still in Preview, so there can be changes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7379bcc1",
   "metadata": {},
   "source": [
    "## KQL Database Deployment Explained\n",
    "\n",
    "The `deploy_kql_database()` function handles the deployment of KQL Databases as child items of Eventhouses using the following approach:\n",
    "\n",
    "### Why Use Fabric API Instead of Import?\n",
    "\n",
    "- **Fabric CLI Limitation**: While `fab create` can create a KQL database structure, `fab import` cannot properly import KQL database content including schema, tables, and functions\n",
    "- **API Solution**: The `fab api` command allows us to send direct REST API calls to the Fabric service for more complete control\n",
    "\n",
    "### Deployment Process:\n",
    "\n",
    "1. **Read Configuration**: Loads `DatabaseProperties.json` to get database settings (type, caching periods, etc.)\n",
    "2. **Update Parent Reference**: Sets the `parentEventhouseItemId` to link the KQL database to its Eventhouse\n",
    "3. **Create Database**: Uses Fabric API POST call to create the KQL database structure\n",
    "4. **Deploy Schema**: Reads `DatabaseSchema.kql` and executes each KQL command individually using the `runKqlCommand` API\n",
    "5. **ID Mapping**: Tracks old/new IDs for cross-reference replacement\n",
    "\n",
    "### Key Features:\n",
    "\n",
    "- **Handles Complex KQL**: Supports tables, functions, materialized views, update policies, and encoding policies\n",
    "- **Error Resilience**: Continues deployment even if individual KQL commands fail (useful for dependencies)\n",
    "- **Parent-Child Relationship**: Maintains proper Eventhouse ↔ KQL Database relationships\n",
    "- **ID Replacement**: Ensures all cross-references use new deployment IDs\n",
    "\n",
    "### Usage in Deployment:\n",
    "\n",
    "The function is automatically called when deploying `.Eventhouse` items that contain `.children/*.KQLDatabase` folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7032812",
   "metadata": {},
   "outputs": [],
   "source": [
    "string = '''{\n",
    "\"databaseType\": \"ReadWrite\",\n",
    "\"parentEventhouseItemId\": \"0ebce58b-207c-4d5c-b4e4-a9da0dc9806c\",\n",
    "\"oneLakeCachingPeriod\": \"P36500D\",\n",
    "\"oneLakeStandardStoragePeriod\": \"P36500D\"\n",
    "}'''"
   ]
  }
 ],
 "metadata": {
  "a365ComputeOptions": null,
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "",
    "default_lakehouse_name": "",
    "default_lakehouse_workspace_id": "",
    "known_lakehouses": [
     {
      "id": ""
     }
    ]
   }
  },
  "kernel_info": {
   "jupyter_kernel_name": "python3.11",
   "name": "jupyter"
  },
  "kernelspec": {
   "display_name": "Jupyter",
   "name": "jupyter"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "jupyter_python",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "sessionKeepAliveTimeout": 0,
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
